{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Before running this notebook, run [Prepare data](Prepare data.ipynb) in order to create `../data/CleanedReviews.pickle`, which is required here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple, OrderedDict\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "\n",
    "sns.set(style=\"white\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figsize = (16, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_utils import ItemSelector, TextStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cat feature_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_reviews = pd.read_pickle('../data/CleanedReviews.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use only reviews which have been up/down-voted.\n",
    "reviews = all_reviews[all_reviews['HelpfulnessDenominator'] > 0].copy()\n",
    "print('Ratio of kept reviews: {:.2f}'.format(len(reviews) / len(all_reviews)))\n",
    "\n",
    "reviews['Helpfulness'] = reviews['HelpfulnessNumerator'].divide(reviews['HelpfulnessDenominator'], axis=0)\n",
    "reviews['Helpful'] = reviews['Helpfulness'] >= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Helpful status:\\n{}'.format(reviews['Helpful'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the plot below, reviews with high scores are more useful than ones with low scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_data = reviews[reviews['HelpfulnessDenominator'] > 10]\n",
    "sns.jointplot(x='Score', y='Helpfulness', data=plot_data, kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section can be used as a template for training several regressors. Each regressor type is trained using a separate procedure (`RandomizedSearchCV` or `GridSearchCV`) and the best estimators are kept.\n",
    "\n",
    "Each search procedure used a pipeline made of two steps:\n",
    "1. Common feature building\n",
    "2. Regressor fitting\n",
    "\n",
    "Parameters for the these two steps are selected indepently for each procedure. Thus, two (best) estimators can use different features.\n",
    "\n",
    "Running the fitting procedure takes a long time. For testing, change `modeling_reviews_n` to select a set of reviews at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modeling_reviews_n = 1000  # number of reviews to select at random\n",
    "test_size=0.8\n",
    "\n",
    "if modeling_reviews_n is not None and len(reviews) > modeling_reviews_n:\n",
    "    rev = reviews.sample(n=modeling_reviews_n, random_state=0)\n",
    "else:\n",
    "    rev = reviews\n",
    "train_data, test_data, train_target, test_target = train_test_split(\n",
    "    rev[['Summary', 'Text', 'Score']], rev['Helpfulness'], random_state=0, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Training with {} samples'.format(len(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the feature processing steps that will be used by all the search procedures. These can be customized by setting parameters for the common transformers in `SearchSettings` objects (see the next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# common features - used by all estimators\n",
    "features_union = FeatureUnion(\n",
    "    \n",
    "    transformer_list=[\n",
    "\n",
    "        # Text field textual features\n",
    "        ('summary_terms',  Pipeline([\n",
    "            ('selector', ItemSelector(key='Summary')),\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "        ])),\n",
    "\n",
    "        # Text field textual features\n",
    "        ('text_terms',  Pipeline([\n",
    "            ('selector', ItemSelector(key='Text')),\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "        ])),\n",
    "        \n",
    "        # Summary field statistics\n",
    "        ('summary_stats', Pipeline([\n",
    "            ('selector', ItemSelector(key='Summary')),\n",
    "            ('stats', TextStats()),\n",
    "            ('vect', DictVectorizer()),\n",
    "        ])),\n",
    "        \n",
    "        # Text field statistics\n",
    "        ('text_stats', Pipeline([\n",
    "            ('selector', ItemSelector(key='Summary')),\n",
    "            ('stats', TextStats()),\n",
    "            ('vect', DictVectorizer()),\n",
    "        ])),\n",
    "        \n",
    "        # Score review\n",
    "        ('score', Pipeline([\n",
    "            ('selector', ItemSelector(key=['Score'])),\n",
    "            ('value', MinMaxScaler()),\n",
    "        ])),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# parameters for common features used in cross validation\n",
    "features_parameters = {\n",
    "    'features__text_terms__tfidf__min_df': (0, 0.1, 0.2),  # ignore terms with document frequency strictly lower\n",
    "    'features__text_terms__tfidf__use_idf': (True, False),\n",
    "    'features__transformer_weights': [{\n",
    "            'summary_terms': 1,\n",
    "            'text_terms': 1,\n",
    "            'summary_stats': 1,\n",
    "            'text_stats': 1,\n",
    "            'score': 1\n",
    "        }, {\n",
    "            'summary_terms': 1,\n",
    "            'text_terms': 0.5,\n",
    "            'summary_stats': 1,\n",
    "            'text_stats': 0.5,\n",
    "            'score': 1\n",
    "        }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a list of search procedures as described above. We use two baselines: a regressor that predicts the mean and one that predicts the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SearchSettings = namedtuple('SearchSettings', [\n",
    "    'name',        # name of the search operation\n",
    "    'estimator',   # estimators for which we test hyperparameters\n",
    "    'parameters',  # hyperparameters that are going to be added to `features_parameters`\n",
    "    'procedure',   # randomized for RandomizedSearchCV, grid for GridSearchCV or\n",
    "                   # dummy for a fake search procedure (train dummy estimator)\n",
    "])\n",
    "\n",
    "search_settings_list = []\n",
    "\n",
    "search_settings_list.append(SearchSettings(\n",
    "    name='Dummy_Mean',\n",
    "    estimator=DummyRegressor(strategy='mean'),\n",
    "    parameters={},\n",
    "    procedure='dummy'\n",
    "))\n",
    "\n",
    "search_settings_list.append(SearchSettings(\n",
    "    name='Dummy_Median',\n",
    "    estimator=DummyRegressor(strategy='median'),\n",
    "    parameters={},\n",
    "    procedure='dummy'\n",
    "))\n",
    "\n",
    "search_settings_list.append(SearchSettings(\n",
    "    name='Ridge',\n",
    "    estimator=Ridge(),\n",
    "    parameters={\n",
    "        'estimator__alpha': [1, 2]\n",
    "    },\n",
    "    procedure='randomized'\n",
    "))\n",
    "\n",
    "search_settings_list.append(SearchSettings(\n",
    "    name='LinearSVR',\n",
    "    estimator=LinearSVR(),\n",
    "    parameters={\n",
    "        'estimator__C': [1, 2]\n",
    "    },\n",
    "    procedure='randomized'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r_s_iter = 10   # number of iterations for RandomizedSearchCV\n",
    "scoring = 'neg_mean_squared_error'  # scoring method used for selecting the hyperparameters\n",
    "verbose = 1  # output level\n",
    "n_jobs = 1  # parallelization level\n",
    "cv = 3  # cross-validation options\n",
    "searches = OrderedDict()\n",
    "\n",
    "for s in search_settings_list:\n",
    "    \n",
    "    print('Fitting {}'.format(s.name))\n",
    "    \n",
    "    pipeline= Pipeline([\n",
    "        ('features', features_union),\n",
    "        ('estimator', s.estimator)\n",
    "    ])\n",
    "    \n",
    "    # merged hyperparameters space\n",
    "    params = {**features_parameters, **s.parameters}\n",
    "    \n",
    "    if s.procedure == 'randomized':\n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline, params,\n",
    "            scoring=scoring,\n",
    "            n_jobs=n_jobs, verbose=verbose, n_iter=r_s_iter, cv=cv)\n",
    "    elif s.procedure == 'grid':\n",
    "        search = GridSearchCV(\n",
    "            pipeline, params,\n",
    "            scoring=scoring,\n",
    "            n_jobs=n_jobs, verbose=verbose, cv=cv)\n",
    "    else:\n",
    "        # we should not do any cross-validation here as we waste time\n",
    "        # the estimator is refitted, so it gives correct results\n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline, {},\n",
    "            scoring=scoring,\n",
    "            n_jobs=n_jobs, verbose=verbose, n_iter=1, cv=2)\n",
    "    \n",
    "    %time search.fit(train_data, train_target)\n",
    "    \n",
    "    searches[s.name] = search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the best estimators that resulted from each procedure, along with their training errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for name, search in searches.items():\n",
    "    print(name, '\\n',\n",
    "          'score:', search.best_score_, '\\n',\n",
    "          'parameters:\\n', search.best_params_,\n",
    "          end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation set metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# regression\n",
    "\n",
    "results = pd.DataFrame(index=searches.keys())\n",
    "\n",
    "for name, search in searches.items():\n",
    "    predictions = search.predict(test_data)\n",
    "    \n",
    "    mse = metrics.mean_squared_error(test_target, predictions)\n",
    "    \n",
    "    plt.hist(predictions)\n",
    "    plt.show()\n",
    "    \n",
    "    predictions[predictions < 0] = 0\n",
    "    predictions[predictions > 1] = 1\n",
    "    mse_bounded = metrics.mean_squared_error(test_target, predictions)\n",
    "    \n",
    "    results.loc[name, 'mse'] = mse\n",
    "    results.loc[name, 'mse_bounded'] = mse_bounded\n",
    "\n",
    "print(results)\n",
    "results.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the best fitted estimators of each type for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_estimators = dict([(name, s.best_estimator_) for name, s in searches.items()])\n",
    "\n",
    "with open('../data/helpfulness_estimators.pickle', 'wb') as f:\n",
    "    pickle.dump(best_estimators, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Add estimators and search paramters.\n",
    "\n",
    "# TODO Save models in their own files.\n",
    "\n",
    "# TODO Refit models using all the data.\n",
    "\n",
    "# TODO Some text fields contain HTML. Add a pipeline step to keep only the content.\n",
    "\n",
    "# Use `linear_model.RidgeCV` instead of `linear_model.Ridge`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
